{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.0 fragment_Voc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3500000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kara_chen/anaconda3/envs/amp/lib/python3.8/site-packages/numpy/core/fromnumeric.py:86: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
      "[09:14:37] Explicit valence for atom # 10 Rn greater than permitted\n",
      "[09:22:38] Explicit valence for atom # 10 Rn greater than permitted\n",
      "[09:42:03] Explicit valence for atom # 10 Rn greater than permitted\n",
      "[09:49:31] Explicit valence for atom # 10 Rn greater than permitted\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Encode a fragment.\n",
    "from rdkit import Chem\n",
    "from fragment_utils.mol_utils import join_fragments\n",
    "from fragment_utils.mol_utils import split_molecule\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "def encode_molecule(m):\n",
    "    fs_list = []\n",
    "    count = 0\n",
    "    error_smiles = []\n",
    "    for index,i in enumerate(m):\n",
    "        try:\n",
    "            fs = [Chem.MolToSmiles(f) for f in split_molecule(Chem.MolFromSmiles(i))]\n",
    "            # encoded = \"-\".join([encodings[f] for f in fs])\n",
    "            fs_list.extend(fs)\n",
    "            # print(index)\n",
    "        except Exception as e:\n",
    "            error_smiles.append(i)\n",
    "  \n",
    "    return fs_list, error_smiles\n",
    "\n",
    "def decode_molecule(enc):\n",
    "    fs_list = []\n",
    "    for index, i in enumerate(enc):\n",
    "        fs = [Chem.MolFromSmiles(x) for x in i]\n",
    "        fs = join_fragments(fs)\n",
    "        fs_list.append(Chem.MolToSmiles(fs))\n",
    "        print(index)\n",
    "    return fs_list\n",
    "\n",
    "def decode_molecule_test(enc):\n",
    "    fs_list = []\n",
    "    fs = [Chem.MolFromSmiles(x) for x in enc]\n",
    "    fs = join_fragments(fs)\n",
    "    fs_list.append(Chem.MolToSmiles(fs))\n",
    "    return fs_list\n",
    "\n",
    "\n",
    "smiles = pd.read_csv('./data/Transformer/01_succeed_smiles.csv',header=None).values.flatten().tolist()\n",
    "smiles = random.sample(smiles,3500000)\n",
    "print(len(smiles))\n",
    "\n",
    "enc,drop_smiles = encode_molecule(smiles)\n",
    "\n",
    "\n",
    "smiles_filtered = list(set(smiles)-set(drop_smiles))\n",
    "pd.DataFrame(smiles_filtered).to_csv('./data/Transformer/data_set.csv',header=None,index=None)\n",
    "\n",
    "with open('./data/Transformer/fragments_counts.txt',\"a\") as f:\n",
    "    f.write(\"the fragment number of data (not drop duplicated):  \" + str(len(enc))+'\\n')\n",
    "    f.write('the smiles number of data:  '+ str(len(smiles_filtered))+'\\n')\n",
    "    f.close()\n",
    "\n",
    "\n",
    "output = list(set(enc))\n",
    "pd.DataFrame(output).to_csv('./data/fragments_Voc2.csv',header=False,index=False)\n",
    "\n",
    "\n",
    "# frg_list = ['C[Yb]',  '[Yb]C1CCN([Lu])CC1', '[Yb]N[Lu]', '[Yb]c1cc([Lu])ccc1[Ta]','[Yb]c1cc2c([Lu])ncnc2[nH]1', '[Yb]O[Lu]', '[Yb]c1ccc2oc([Lu])nc2c1', '[Yb]N[Lu]', '[Yb]c1ccc([Lu])cc1', 'Cl[Yb]', 'C[Yb]']\n",
    "\n",
    "# dec = decode_molecule_test(enc)\n",
    "\n",
    "# print(dec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.0 训练集与测试集数据拆分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train List: 2799435\n",
      "Validation List: 349929\n",
      "Test List: 349930\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "\n",
    "\n",
    "# 从CSV文件中加载数据集\n",
    "data = pd.read_csv('./data/Transformer/data_set.csv').values.flatten().tolist()\n",
    "\n",
    "\n",
    "\n",
    "def split_list(input_list):\n",
    "    # 随机打乱列表\n",
    "    random.shuffle(input_list)\n",
    "\n",
    "    # 计算每个部分的大小\n",
    "    n = len(input_list)\n",
    "    train_size = int(0.8 * n)\n",
    "    valid_size = int(0.1 * n)\n",
    "    \n",
    "    # 切分列表\n",
    "    train_list = input_list[:train_size]\n",
    "    valid_list = input_list[train_size:train_size + valid_size]\n",
    "    test_list = input_list[train_size + valid_size:]\n",
    "\n",
    "    return train_list, valid_list, test_list\n",
    "\n",
    "\n",
    "# 拆分列表\n",
    "train, valid, test = split_list(data)\n",
    "\n",
    "print(\"Train List:\", len(train))\n",
    "print(\"Validation List:\", len(valid))\n",
    "print(\"Test List:\", len(test))\n",
    "\n",
    "pd.DataFrame(train,columns=['smiles']).to_csv('./data/gen_train.csv')\n",
    "pd.DataFrame(valid,columns=['smiles']).to_csv('./data/gen_valid.csv')\n",
    "pd.DataFrame(test,columns=['smiles']).to_csv('./data/gen_test.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
